import asyncio
import time
from typing import List, Tuple, Any, Callable
from dataclasses import dataclass
from fastapi import HTTPException, status

from src.core.config import BATCH_SIZE, BATCH_TIMEOUT_MS, MAX_QUEUE_SIZE
from src.models.schemas import EmbeddingRequest, BGEEmbeddingResponse
from src.core.logging_config import get_logger

logger = get_logger("batch_service")

@dataclass
class BatchItem:
    """–≠–ª–µ–º–µ–Ω—Ç –±–∞—Ç—á–∞ - –∑–∞–ø—Ä–æ—Å –∏ future –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
    request: EmbeddingRequest
    future: asyncio.Future
    timestamp: float
    original_input_length: int  # –î–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ä–∞–∑–±–æ—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

class BatchProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –±–∞—Ç—á–µ–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    def __init__(self):
        self.pending_requests: List[BatchItem] = []
        self.batch_lock = asyncio.Lock()
        self.processing_semaphore = asyncio.Semaphore(2)  # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π
        self.stats = {
            "total_batches": 0,
            "total_requests": 0,
            "avg_batch_size": 0,
            "last_batch_time": 0
        }
        # Graceful shutdown support
        self.is_shutting_down = False
        self.shutdown_timeout = 30.0  # seconds
        self.active_batches = set()  # Track active batch processing
    
    async def add_request(self, request: EmbeddingRequest, process_func: Callable) -> Any:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ –±–∞—Ç—á –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç"""
        # Check if we're shutting down
        if self.is_shutting_down:
            logger.warning("Rejecting new request during shutdown")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Service is shutting down, please try again later"
            )
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if isinstance(request.input, str):
            inputs = [request.input]
        elif isinstance(request.input, list):
            if not request.input:
                # –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ä–∞–∑—É
                return await process_func(request, None)
            elif all(isinstance(item, int) for item in request.input):
                inputs = [" ".join(map(str, request.input))]
            else:
                inputs = request.input
        else:
            inputs = [str(request.input)]
        
        # –°–æ–∑–¥–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç –±–∞—Ç—á–∞
        future = asyncio.Future()
        batch_item = BatchItem(
            request=EmbeddingRequest(
                model=request.model,
                input=inputs,
                encoding_format=request.encoding_format,
                return_dense=getattr(request, 'return_dense', True),
                return_sparse=getattr(request, 'return_sparse', True),
                return_colbert=getattr(request, 'return_colbert', True),
                user=request.user
            ),
            future=future,
            timestamp=time.time(),
            original_input_length=len(inputs)
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å –±–∞—Ç—á–∏–Ω–≥–∞
        async with self.batch_lock:
            if len(self.pending_requests) >= MAX_QUEUE_SIZE:
                raise HTTPException(
                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                    detail="Server is currently handling maximum number of requests. Please try again later."
                )
            
            self.pending_requests.append(batch_item)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –±–∞—Ç—á
            should_process = (
                len(self.pending_requests) >= BATCH_SIZE or
                (self.pending_requests and 
                 (time.time() - self.pending_requests[0].timestamp) * 1000 >= BATCH_TIMEOUT_MS)
            )
            
            if should_process:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞—Ç—á –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                batch_to_process = self.pending_requests[:BATCH_SIZE]
                self.pending_requests = self.pending_requests[BATCH_SIZE:]
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –±–∞—Ç—á–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
                asyncio.create_task(self._process_batch(batch_to_process, process_func))
        
        # –ñ–¥–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        return await future
    
    async def _process_batch(self, batch: List[BatchItem], process_func: Callable):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –∑–∞–ø—Ä–æ—Å–æ–≤"""
        batch_id = id(batch)
        self.active_batches.add(batch_id)
        
        try:
            async with self.processing_semaphore:
                start_time = time.time()
                logger.info("Processing batch started", extra={
                    "batch_id": batch_id,
                    "batch_size": len(batch),
                    "total_texts": sum(len(item.request.input) for item in batch)
                })
                
                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –≤ –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π –∑–∞–ø—Ä–æ—Å
                all_texts = []
                request_boundaries = []  # –ì—Ä–∞–Ω–∏—Ü—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –æ–±—â–µ–º —Å–ø–∏—Å–∫–µ
                
                current_index = 0
                for item in batch:
                    start_idx = current_index
                    all_texts.extend(item.request.input)
                    current_index += len(item.request.input)
                    request_boundaries.append((start_idx, current_index))
                
                print(f"üîÑ Processing batch: {len(batch)} requests, {len(all_texts)} texts total")
                
                # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ–ª–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                first_request = batch[0].request
                return_dense = getattr(first_request, 'return_dense', True)
                return_sparse = getattr(first_request, 'return_sparse', True)
                return_colbert = getattr(first_request, 'return_colbert', True)
                
                combined_request = EmbeddingRequest(
                    model=first_request.model,
                    input=all_texts,
                    encoding_format=first_request.encoding_format,
                    return_dense=return_dense,
                    return_sparse=return_sparse,
                    return_colbert=return_colbert,
                    user=getattr(first_request, 'user', None)
                )
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–µ—Å—å –±–∞—Ç—á –æ–¥–Ω–∏–º –≤—ã–∑–æ–≤–æ–º
                from src.main import app
                batch_result = await process_func(combined_request, app.state.encoder)
                
                # –†–∞–∑–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –∑–∞–ø—Ä–æ—Å–∞–º
                for i, (item, (start_idx, end_idx)) in enumerate(zip(batch, request_boundaries)):
                    try:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (BGE —Ñ–æ—Ä–º–∞—Ç)
                        request_data = batch_result.data[start_idx:end_idx]
                        # –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º
                        for j, data_item in enumerate(request_data):
                            data_item.index = j
                        
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                        orig_req = item.request
                        requested_types = []
                        if getattr(orig_req, 'return_dense', True):
                            requested_types.append("dense")
                        if getattr(orig_req, 'return_sparse', True):
                            requested_types.append("sparse")
                        if getattr(orig_req, 'return_colbert', True):
                            requested_types.append("colbert")
                        
                        individual_result = BGEEmbeddingResponse(
                            data=request_data,
                            model=batch_result.model,
                            usage={
                                "prompt_tokens": int(sum(len(text.split()) * 1.3 for text in item.request.input)),
                                "total_tokens": int(sum(len(text.split()) * 1.3 for text in item.request.input))
                            },
                            embedding_types=requested_types
                        )

                        
                        item.future.set_result(individual_result)
                    except Exception as e:
                        print(f"Error processing individual result {i}: {e}")
                        item.future.set_exception(e)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                batch_time = time.time() - start_time
                self.stats["total_batches"] += 1
                self.stats["total_requests"] += len(batch)
                self.stats["avg_batch_size"] = self.stats["total_requests"] / self.stats["total_batches"]
                self.stats["last_batch_time"] = batch_time
                
                logger.info("Batch processing completed", extra={
                    "batch_id": batch_id,
                    "batch_size": len(batch),
                    "processing_time": round(batch_time, 2)
                })
                
        except Exception as e:
            logger.error("Batch processing failed", extra={
                "batch_id": batch_id,
                "error": str(e)
            }, exc_info=True)
            # –£–≤–µ–¥–æ–º–ª—è–µ–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –æ–± –æ—à–∏–±–∫–µ
            for item in batch:
                if not item.future.done():
                    item.future.set_exception(e)
        finally:
            # Remove from active batches
            self.active_batches.discard(batch_id)
    
    async def start_timeout_processor(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ç–∞–π–º–∞—É—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ–ø–æ–ª–Ω—ã—Ö –±–∞—Ç—á–µ–π"""
        while True:
            try:
                await asyncio.sleep(BATCH_TIMEOUT_MS / 1000)  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–µ–∫—É–Ω–¥—ã
                
                async with self.batch_lock:
                    if (self.pending_requests and 
                        (time.time() - self.pending_requests[0].timestamp) * 1000 >= BATCH_TIMEOUT_MS):
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –æ–∂–∏–¥–∞—é—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã
                        batch_to_process = self.pending_requests[:]
                        self.pending_requests.clear()
                        
                        print(f"‚è∞ Timeout batch: {len(batch_to_process)} requests")
                        
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ —Ç–∏–ø—É –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º BGE –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è —Ç–∞–π–º–∞—É—Ç –±–∞—Ç—á–µ–π
                        from src.services.model_service import process_bge_embeddings
                        asyncio.create_task(self._process_batch(batch_to_process, process_bge_embeddings))
                        
            except Exception as e:
                logger.error("Error in timeout processor", extra={"error": str(e)}, exc_info=True)
                await asyncio.sleep(1)
    
    async def graceful_shutdown(self):
        """Gracefully shutdown the batch processor"""
        logger.info("Starting graceful shutdown", extra={
            "pending_requests": len(self.pending_requests),
            "active_batches": len(self.active_batches),
            "shutdown_timeout": self.shutdown_timeout
        })
        
        # Stop accepting new requests
        self.is_shutting_down = True
        
        # Process remaining requests in queue
        if self.pending_requests:
            logger.info("Processing remaining requests in queue", extra={
                "remaining_count": len(self.pending_requests)
            })
            
            async with self.batch_lock:
                if self.pending_requests:
                    batch_to_process = self.pending_requests[:]
                    self.pending_requests.clear()
                    
                    # Process final batch
                    from src.services.model_service import process_bge_embeddings
                    await self._process_batch(batch_to_process, process_bge_embeddings)
        
        # Wait for active batches to complete
        if self.active_batches:
            logger.info("Waiting for active batches to complete", extra={
                "active_count": len(self.active_batches)
            })
            
            start_time = time.time()
            while self.active_batches and (time.time() - start_time) < self.shutdown_timeout:
                await asyncio.sleep(0.1)
            
            if self.active_batches:
                logger.warning("Some batches did not complete within timeout", extra={
                    "remaining_batches": len(self.active_batches),
                    "timeout": self.shutdown_timeout
                })
            else:
                logger.info("All active batches completed successfully")
        
        logger.info("Graceful shutdown completed")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –±–∞—Ç—á–µ–π
batch_processor = BatchProcessor()

async def enqueue_batch_request(request: EmbeddingRequest, process_func: Callable) -> Any:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ –±–∞—Ç—á –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    return await batch_processor.add_request(request, process_func)

def get_batch_stats():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞—Ç—á–∏–Ω–≥–∞"""
    return batch_processor.stats
