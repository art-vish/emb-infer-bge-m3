# emb-infer-bge-m3: BGE-M3 Embedding Inference API
# Production version using pre-built Docker image from Docker Hub

services:
  emb-infer-bge-m3:
    image: asvishnya/emb-infer-bge-m3:latest
    ports:
      - "8000:8000"
    environment:
      # Security
      - API_TOKEN=${API_TOKEN:-your_api_token_here}
      
      # Performance tuning
      - MAX_QUEUE_SIZE=${MAX_QUEUE_SIZE:-50}
      - PROCESSING_CONCURRENCY=${PROCESSING_CONCURRENCY:-2}
      - BATCH_SIZE=${BATCH_SIZE:-8}
      - BATCH_TIMEOUT_MS=${BATCH_TIMEOUT_MS:-100}
      
      # Model configuration
      - MODEL_PATH=/app/BGE-M3
      - MODEL_NAME=${MODEL_NAME:-BAAI/bge-m3}
      
      # GPU configuration
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      # Mount for Hugging Face cache to persist downloads
      - ~/.cache/huggingface:/root/.cache/huggingface
      # Optional: uncomment if you have local BGE-M3 model files
      # - ./BGE-M3:/app/BGE-M3
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
